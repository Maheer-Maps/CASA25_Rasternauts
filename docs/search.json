[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nepal Landslides Risk Application",
    "section": "",
    "text": "The project develops an application using Google Earth Engine for policymakers to assess landslide susceptibility in Nepal. The model identifies high-hazard zones using terrain, hydrology, soil, and land cover data, following Hong et al. (2007). Additionally, a machine learning model is tested for accuracy using recent landslide points from the Bipad Portal (6–13 April 2025). This dual approach allows comparisons between knowledge-based and data-driven methods. The web platform improves hazard communication and supports sustainable landslide inventory mapping, addressing gaps in data-sharing and stakeholder collaboration highlighted by Meena et al. (2021), crucial for Nepal’s disaster management framework.\n\n\n\nGlobal Modelling by Hong et al., 2007\n\n\n\n\n\nAlmost 80% of Nepal is prone to landslides (Department of Water Induced Disaster Prevention, 2015), creating an urgent need for accessible research on susceptibility. Updating research to identify key factors provides critical insights into vulnerable populations and infrastructure. Yet, outputs often remain disconnected from local needs. Our application addresses this gap by offering an intuitive, Nepali-language platform for policymakers, planners, and stakeholders to visualise high-susceptibility zones. By bridging academic research and local administration, it supports evidence-based mitigation and sustainable landslide inventory mapping (Meena et al., 2021), helping overcome long-standing issues in disaster data sharing, cooperation, and resilience planning.\n\n\n\nOur platform is designed for hybrid users across academia and administrative bodies involved in landslide management in Nepal. These users need scientifically robust yet accessible tools to inform policies and interventions. Historically, gaps between research and local awareness limited practical outcomes (Meena et al., 2021). By delivering models through a Nepali-language website rather than academic papers, we improve accessibility, usability, and real-world impact. The platform also promotes stakeholder collaboration, strengthening Nepal’s capacity for disaster preparedness, response, and long-term landslide risk reduction (Chalise et al., 2022)."
  },
  {
    "objectID": "index.html#project-summary",
    "href": "index.html#project-summary",
    "title": "Nepal Landslides Risk Application",
    "section": "",
    "text": "The project develops an application using Google Earth Engine for policymakers to assess landslide susceptibility in Nepal. The model identifies high-hazard zones using terrain, hydrology, soil, and land cover data, following Hong et al. (2007). Additionally, a machine learning model is tested for accuracy using recent landslide points from the Bipad Portal (6–13 April 2025). This dual approach allows comparisons between knowledge-based and data-driven methods. The web platform improves hazard communication and supports sustainable landslide inventory mapping, addressing gaps in data-sharing and stakeholder collaboration highlighted by Meena et al. (2021), crucial for Nepal’s disaster management framework.\n\n\n\nGlobal Modelling by Hong et al., 2007\n\n\n\n\n\nAlmost 80% of Nepal is prone to landslides (Department of Water Induced Disaster Prevention, 2015), creating an urgent need for accessible research on susceptibility. Updating research to identify key factors provides critical insights into vulnerable populations and infrastructure. Yet, outputs often remain disconnected from local needs. Our application addresses this gap by offering an intuitive, Nepali-language platform for policymakers, planners, and stakeholders to visualise high-susceptibility zones. By bridging academic research and local administration, it supports evidence-based mitigation and sustainable landslide inventory mapping (Meena et al., 2021), helping overcome long-standing issues in disaster data sharing, cooperation, and resilience planning.\n\n\n\nOur platform is designed for hybrid users across academia and administrative bodies involved in landslide management in Nepal. These users need scientifically robust yet accessible tools to inform policies and interventions. Historically, gaps between research and local awareness limited practical outcomes (Meena et al., 2021). By delivering models through a Nepali-language website rather than academic papers, we improve accessibility, usability, and real-world impact. The platform also promotes stakeholder collaboration, strengthening Nepal’s capacity for disaster preparedness, response, and long-term landslide risk reduction (Chalise et al., 2022)."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Nepal Landslides Risk Application",
    "section": "Data",
    "text": "Data\n\n\n\nCategory\n\n\nDataset\n\n\nDescription\n\n\nSource\n\n\n\n\nGEE\n\n\nElevation/Slope\n\n\nNASA SRTM Digital Elevation 30m\n\n\nLink\n\n\n\n\nLand Cover\n\n\nMODIS Land Cover Type (2020)\n\n\nLink\n\n\n\n\nHydrology\n\n\nWWF HydroSHEDS flow accumulation\n\n\nLink\n\n\n\n\nSoil Texture\n\n\nOpenLandMap Soil Texture Class (USDA System)\n\n\nLink\n\n\n\n\nSoil Type\n\n\nOpenLandMap Clay Content\n\n\nLink\n\n\n\n\nPopulation Density\n\n\nGlobal Human Settlement Layer of spatial distribution of residential population\n\n\nLink\n\n\n\n\nOther\n\n\nHistorical Landslides\n\n\nOfficial reported landslide incident points (2011 - April 2025)\n\n\nLink\n\n\n\n\nCensus Population\n\n\nOfficial Population per District 2021 Census\n\n\nLink\n\n\n\n\nCountry Boundary\n\n\nOfficial boundary layer for Nepal\n\n\nLink\n\n\n\n\nDistricts Boundaries\n\n\nOfficial bouundaries of Nepal’s 77 districts\n\n\nLink"
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Nepal Landslides Risk Application",
    "section": "Methodology",
    "text": "Methodology\n\nSusceptibility Model\nTo recreate Hong et al.’s susceptibility model in GEE, we imported six environmental variables into a weighted linear combination. Each variable was normalized on a 0-1 scale, and in combination with the weights provided a susceptibility score for each pixel. Susceptibility was then normalized 0-1 for the whole of Nepal.\n\n\n\nSusceptibility Model Methodology\n\n\n\nSusceptibility Model Code\n\n // Import shapefile for country boundaries\nvar filteredCountries = ee.FeatureCollection(\"projects/ee-testing-casa-25/assets/Nepal_boundary\");\nMap.centerObject(filteredCountries, 7);\n\n// DEM\nvar dem = ee.Image('USGS/SRTMGL1_003');\nvar clippedDem = dem.clip(filteredCountries);\n\n// Variable 1: SLOPE\nvar slope = ee.Terrain.slope(clippedDem);\nvar minMaxSlope = slope.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: filteredCountries.geometry(),\n  scale: 250,\n  bestEffort: true\n});\nvar minSlope = ee.Number(minMaxSlope.get('slope_min'));\nvar maxSlope = ee.Number(minMaxSlope.get('slope_max'));\n\nvar normSlope = slope.subtract(minSlope).divide(maxSlope.subtract(minSlope));\nMap.addLayer(normSlope, {min: 0, max: 1, palette: ['white', 'blue', 'green', 'yellow', 'red']}, 'Nomalised Slope', false);\n\n// Variable 2: ELEVATION\nvar elev = clippedDem;\nvar minMaxElevation = elev.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: filteredCountries.geometry(),\n  scale: 250,\n  bestEffort: true\n});\nvar minElev = ee.Number(minMaxElevation.get('elevation_min'));\nvar maxElev = ee.Number(minMaxElevation.get('elevation_max'));\n\nvar normElev = elev.subtract(minElev).divide(maxElev.subtract(minElev));\nMap.addLayer(normElev, {min: 0, max: 1, palette: ['white', 'blue', 'green', 'yellow', 'red']}, 'Nomalised Elevation', false);\n\n// Variable 3: MODIS land cover type and reclassification\nvar landcover = ee.ImageCollection(\"MODIS/061/MCD12Q1\")\n  .select(\"LC_Type1\")\n  .filterDate(\"2023-01-01\", \"2023-12-31\")\n  .first()\n  .clip(filteredCountries);\n\nvar reclassifiedLand = landcover.remap(\n  [0, 15, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 13, 17],\n  [0, 0, 0.1, 0.1, 0.1, 0.2, 0.2, 0.3, 0.4, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.0]\n);\n\nvar minMaxLand = reclassifiedLand.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: filteredCountries.geometry(),\n  scale: 250,\n  bestEffort: true\n});\nvar minLand = ee.Number(minMaxLand.get('remapped_min'));\nvar maxLand = ee.Number(minMaxLand.get('remapped_max'));\n\nMap.addLayer(reclassifiedLand, {min: 0, max: 1, palette: ['blue', 'green', 'yellow', 'red']}, \"Reclassified Land Cover\", false);\n\n// Variable 4a: Drainage Density\nvar flowAccum = ee.Image(\"WWF/HydroSHEDS/15ACC\").clip(filteredCountries);\nvar kernel = ee.Kernel.circle(5000, 'meters');\nvar threshold = 50;\nvar streamMask = flowAccum.gt(threshold);\nvar drainageDensity = streamMask.reduceNeighborhood({\n  reducer: ee.Reducer.mean(),\n  kernel: kernel\n});\nvar minMaxDensity = drainageDensity.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: filteredCountries.geometry(),\n  scale: 250,\n  bestEffort: true\n});\nvar minDensity = ee.Number(minMaxDensity.get('b1_mean_min'));\nvar maxDensity = ee.Number(minMaxDensity.get('b1_mean_max'));\n\nvar normalizedDrainageDensity = drainageDensity.subtract(minDensity).divide(maxDensity.subtract(minDensity));\nMap.addLayer(normalizedDrainageDensity, {min: 0, max: 1, palette: ['white', 'green', 'blue']}, 'Normalised Drainage Density', false);\n\n// Variable 4b: Distance to Drainage\nvar streams = flowAccum.gt(threshold).selfMask();\nvar distanceToDrainage = streams.fastDistanceTransform().sqrt().clip(filteredCountries);\nvar minMax = distanceToDrainage.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: filteredCountries.geometry(),\n  scale: 250,\n  bestEffort: true\n});\nvar minDist = ee.Number(minMax.get('distance_min'));\nvar maxDist = ee.Number(minMax.get('distance_max'));\n\nvar normalizedDistance = distanceToDrainage.subtract(minDist).divide(maxDist.subtract(minDist));\nnormalizedDistance = ee.Image(1).subtract(normalizedDistance);\nMap.addLayer(normalizedDistance, {min: 0, max: 1, palette: [\"blue\", \"yellow\", \"red\"]}, \"Normalized Distance to Drainage\", false);\n\n// Variable 5: Soil Texture (Original)\nvar soilTexture = ee.Image(\"OpenLandMap/SOL/SOL_TEXTURE-CLASS_USDA-TT_M/v02\").select(\"b0\").clip(filteredCountries);\nvar reclassifiedSoil = soilTexture.remap(\n [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n [3, 3, 3, 3, 2, 2, 2, 2, 1, 2, 1, 1]\n);\nreclassifiedSoil = ee.Image(4).subtract(reclassifiedSoil);\nvar minMaxSoilTexture = reclassifiedSoil.reduceRegion({\n reducer: ee.Reducer.minMax(),\n geometry: filteredCountries.geometry(),\n scale: 250,\n bestEffort: true\n});\n\nvar minSoil = ee.Number(minMaxSoilTexture.get('constant_min'));\nvar maxSoil = ee.Number(minMaxSoilTexture.get('constant_max'));\n\nvar normalizedSoilTexture = reclassifiedSoil.subtract(minSoil).divide(maxSoil.subtract(minSoil));\nMap.addLayer(normalizedSoilTexture, {min: 0, max: 1, palette: [\"yellow\", \"green\", \"red\"]}, \"Normalised Soil Texture\", false);\n\n// Variable 6: Soil Type (Clay %)\nvar soilType = ee.Image(\"OpenLandMap/SOL/SOL_CLAY-WFRACTION_USDA-3A1A1A_M/v02\").clip(filteredCountries);\nsoilType = soilType.reduce(ee.Reducer.mean());\nvar minMaxClay = soilType.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: filteredCountries.geometry(),\n  scale: 250,\n  bestEffort: true\n});\n\nvar minClay = ee.Number(minMaxClay.get('mean_min'));\nvar maxClay = ee.Number(minMaxClay.get('mean_max'));\n\nvar normClay = soilType.subtract(minClay).divide(maxClay.subtract(minClay));\nMap.addLayer(normClay.select(0), {min: 0, max: 1, palette: [\"blue\", \"yellow\", \"red\"]}, \"Normalized Clay Percentage\", false);\n\n// Resample all layers\nvar targetScale = 1000;\nvar crs = 'EPSG:4326';\nnormSlope = normSlope.reproject({crs: crs, scale: targetScale});\nnormalizedDrainageDensity = normalizedDrainageDensity.reproject({crs: crs, scale: targetScale});\nnormalizedDistance = normalizedDistance.reproject({crs: crs, scale: targetScale});\nnormElev = normElev.reproject({crs: crs, scale: targetScale});\nnormClay = normClay.reproject({crs: crs, scale: targetScale});\nreclassifiedLand = reclassifiedLand.reproject({crs: crs, scale: targetScale});\nnormalizedSoilTexture = normalizedSoilTexture.reproject({crs: crs, scale: targetScale});\n\n// Landslide Susceptibility Model\nvar landslideRisk = normSlope.multiply(0.3)\n  .add(normalizedDrainageDensity.multiply(0.1))\n  // .add(normalizedDistance.multiply(0.1)) // optional\n  .add(normElev.multiply(0.1))\n  .add(normClay.multiply(0.2))\n  .add(reclassifiedLand.multiply(0.1))\n  .add(normalizedSoilTexture.multiply(0.2));\n\n// Rename the band to \"risk\"\nvar landslideRisk = landslideRisk.rename('risk');\n\nvar minMaxRiskLandslide = landslideRisk.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: filteredCountries.geometry(),\n  scale: 250,\n  bestEffort: true\n});\n\nvar minRisk = ee.Number(minMaxRiskLandslide.get('risk_min'));\nvar maxRisk = ee.Number(minMaxRiskLandslide.get('risk_max'));\nvar normalizedLandslideRisk = landslideRisk.subtract(minRisk).divide(maxRisk.subtract(minRisk));\n\nMap.addLayer(normalizedLandslideRisk.select(0), {\n  min: 0,\n  max: 1,\n  palette: ['green', 'yellow', 'red']\n}, \"Normalized Landslide Risk\");\n\n\n\n\nRandom Forest Model\nTo find populated areas most susceptible to landslides, we masked our environmental variables to populated areas, and trained a random forest classifier on landslide and randomly generated non-landslide points from Bipad Portal. Each pixel was scored based on the proportion of decision trees voting landslide.\n\n\n\nRandom Forest Model Methodology\n\n\n\nRandom Forest Model Code\n\n\n// Population Integration\n\n// Load the Global Human Settlement Layer (GHSL) Population ImageCollection\nvar ghslPop = ee.ImageCollection('JRC/GHSL/P2023A/GHS_POP');\n\n// Select the image corresponding to the year 2020 (index 9 in the collection)\nvar pop2020 = ghslPop.toList(12).get(9);\n// Extract the 'population_count' band and clip it to Nepal's boundary\nvar popImage = ee.Image(pop2020).select('population_count').clip(NepalBoundary);\n\n\n// Reproject the population image to 1 km resolution and EPSG:4326 to ensure alignment with the landslide susceptibility model\nvar pop1km = popImage\n  .reproject({\n    crs: crs, // 'EPSG:4326' defined earlier \n    scale: targetScale // 1000 meters defined earlier\n  })\n  .clip(NepalBoundary);\n\n\n//  Visualise the 1km population layer on the map\n//   - Pixels with a population density ≥ 500 people/km² will appear fully red.\n//   - Pixels with lower densities are stretched linearly between white (0) and red (500).\nMap.addLayer(pop1km, {min: 0, max: 500, palette: ['white', 'blue', 'red']}, 'GHSL Population (1km)', false);\n\n// Create a binary population mask\n// 0: No population\n// 1: Some population (any value &gt; 0)\nvar popBinary = pop1km.gt(0)\n  .rename('popBinary')  // rename band for clarity\n  .clip(NepalBoundary);\n\n// Add the binary layer to the map\nMap.addLayer(popBinary, {\n  min: 0,\n  max: 1,\n  palette: ['white', 'red']\n}, 'Binary Population Mask (0/1)', false);\n\n// Visualise the susceptibility model that excludes unpopulated regions\n\n// Mask landslide risk to populated areas only\nvar populatedRisk = landslideRisk.updateMask(popBinary);\n\n// Recompute min and max values after masking\nvar minMaxPopulatedRisk = populatedRisk.reduceRegion({\n  reducer: ee.Reducer.minMax(),\n  geometry: NepalBoundary.geometry(),\n  scale: 1000,\n  bestEffort: true\n});\n\n// Extract new min and max\nvar minRiskPop = ee.Number(minMaxPopulatedRisk.get('risk_min'));\nvar maxRiskPop = ee.Number(minMaxPopulatedRisk.get('risk_max'));\n\n\n// Re-normalise the masked risk layer\nvar normalizedPopulatedRisk = populatedRisk.subtract(minRiskPop)\n  .divide(maxRiskPop.subtract(minRiskPop))\n  .rename('risk');\n\n// Add the new population-masked susceptibility map to the map\nMap.addLayer(normalizedPopulatedRisk, {\n  min: 0,\n  max: 1,\n  palette: ['green', 'yellow', 'red']\n}, 'Normalized Landslide Risk (Populated Areas Only)', false);\n\n// Machine Learning & Evaluation (Dan)\n\n// 1. Classify landslide susceptibility zones\n// select the first band once and use it everywhere\nvar risk = normalizedLandslideRisk.select(0);\n\n// calculate percentiles (tertiles)\nvar susceptibilityPercentiles = risk.reduceRegion({\n  reducer: ee.Reducer.percentile([33, 66]),\n  geometry: NepalBoundary.geometry(),\n  scale: 250,\n  bestEffort: true\n});\n\n// extract break values\nvar p33 = ee.Number(susceptibilityPercentiles.get('risk_p33'));\nvar p66 = ee.Number(susceptibilityPercentiles.get('risk_p66'));\n\n// reclassify ones based on percentiles\nvar susceptibilityZones = normalizedLandslideRisk\n  .where(normalizedLandslideRisk.lt(p33), 1)\n  .where(normalizedLandslideRisk.gte(p33).and(normalizedLandslideRisk.lt(p66)), 2)\n  .where(normalizedLandslideRisk.gte(p66), 3)\n  .rename('zone')\n  .updateMask(normalizedLandslideRisk.mask())\n  .clip(NepalBoundary);\n\n// Visualise susceptibilityZones layer on the map\nMap.addLayer(susceptibilityZones, {min: 1, max: 3, palette: ['green', 'yellow', 'red']}, 'Susceptibility Zones (Quantile)', false);\n\n// Get pixel count per risk class\nvar classCounts = susceptibilityZones.reduceRegion({\n  reducer: ee.Reducer.frequencyHistogram(),\n  geometry: NepalBoundary.geometry(),\n  scale: 250,\n  bestEffort: true\n});\n\n// 2. Overlay landslide points onto Susceptibility Zones and Evaluate\n// Source: https://bipadportal.gov.np/incidents/\nvar landslidePoints = ee.FeatureCollection(\"users/jihunnadi65/landslides_data_v1\");\n\n// Filter to include only points with reported damage\n// Using only damaged landslides may not be optimal here, since our model aims to capture overall landslide susceptibility\n// Including only damaged events could intrduce spatial bias for urban areas and compromise predictive power\n// Instead, we retain all points for evaluation and use damage metrics as supplementary metadata\nvar damagedLandslidePoints = landslidePoints.filter(\n  ee.Filter.or(\n    ee.Filter.gt('peopleDeathCount', 0),\n    ee.Filter.gt('peopleMissingCount', 0),\n    ee.Filter.gt('peopleInjuredCount', 0),\n    ee.Filter.gt('peopleAffectedCount', 0),\n    ee.Filter.gt('infrastructureDestroyedCount', 0)\n  )\n);\n\n// Optionally filter by year (e.g., to match land cover data or recent terrain condition)\n// Filter from 2020 onwards reduces sample size from ~4300 to ~2670 points\n// var recentLandslides = landslidePoints.filter(ee.Filter.gte('year', 2020));\n// Map.addLayer(recentLandslides, {color: 'orange'}, 'Recent Landslides (2020–2025)');\n\n// Visualise all known landslide points\n// Total = 4,300 points (2011-05-14 to 2025-03-24)\nMap.addLayer(landslidePoints, {pointSize: 5, color: 'red'}, 'Landslide Points', false);\n\n// Visualise only landslide points with reported damage\n// Total = 1,994 points in same date range\nMap.addLayer(damagedLandslidePoints, {pointSize: 5, color: 'Orange'}, 'Damaged Landslide Points', false);\n\n// Sample the susceptibility zone for each landslide point\n// Using scale: 1000 to match the resolution of susceptibility raster (defined earlier as targetScale = 1000)\nvar validationSamples = susceptibilityZones.sampleRegions({\n  collection: landslidePoints,\n  scale: 1000, // match the resolution of susceptibility raster\n  geometries: true // retain the original point geometry\n});\n\n// Count number of landslide points in each zone (low = 1, medium = 2, high = 3)\nvar pointCounts = validationSamples.reduceColumns({\n  selectors: ['zone'],\n  reducer: ee.Reducer.frequencyHistogram()\n});\n\n// Calculate proportions for comparion\nvar countsDict = ee.Dictionary(pointCounts.get('histogram'));\nvar totalPoints = countsDict.values().reduce(ee.Reducer.sum());\nvar proportions = countsDict.map(function(key, value) {\n  return ee.Number(value).divide(totalPoints).multiply(100);\n});\n\n// Control test: Generate random points across Nepal to compare spatial bias\n// This helps determine whether landslides fall in high-risk zones more than expected by chance\nvar randomPoints = ee.FeatureCollection.randomPoints({\n  region: NepalBoundary.geometry(),\n  points: landslidePoints.size(), // match number of real landslide points\n  seed: 42 // use a fixed seed (42) to ensure reproducibility of random point generation across runs\n});\n\n// Sample susceptibility zone at random points\nvar randomSamples = susceptibilityZones.sampleRegions({\n  collection: randomPoints,\n  scale: 1000,\n  geometries: true\n});\n\n// Histogram of real landslide zones\nvar realCounts = susceptibilityZones.sampleRegions({\n  collection: landslidePoints,\n  scale: 1000\n}).reduceColumns({\n  selectors: ['zone'],\n  reducer: ee.Reducer.frequencyHistogram()\n});\n\n// Histogram of control group (random control points)\nvar randomCounts = randomSamples.reduceColumns({\n  selectors: ['zone'],\n  reducer: ee.Reducer.frequencyHistogram()\n});\n\n\n// 3.Chi-Square Test for Statistical Significance\n// This test checks whether the distribution of landslide points across rik zones\n// significantly differs from what would be expected by chance (random distribution)\n\n// Extract observed (real) and expected (random) point distribution by zone\nvar realDict = ee.Dictionary(realCounts.get('histogram'));\nvar randomDict = ee.Dictionary(randomCounts.get('histogram'));\n\n\n// Apply Chi-Square formula: sum((O - E)^2 / E) for each risk class\nvar keys = realDict.keys(); // ['1', '2', '3']\nvar chiSquare = keys.map(function(key) {\n  key = ee.String(key);\n  var observed = ee.Number(realDict.get(key));\n  var expected = ee.Number(randomDict.get(key)).max(1); // prevision division by 0\n  return observed.subtract(expected).pow(2).divide(expected);\n});\nvar chiSquareTotal = chiSquare.reduce(ee.Reducer.sum());\n\n// With 3 susceptibility classes, degree of freedom (df)= 2\n// Reference critical values\n// 95% confidence: 5.991, 99%: 9.210, 99.9%: 13.82\n// Chi-square value of 835.16 is significantly higher than all reference thresholds for statistical significance\n\n// 4. Binary Evaluation Using Confusion Matrix\n\n// Build a Binary Confusion Matrix (High Risk = Zone 3, Not High Risk = Zone 1 + 2)\n// Did a landslide point fall in a high-risk zone (True Positive)?\n// Did a random (non-landslide) point fall in a high-risk zone (False Positive)?\n\n// Assign class = 1 to actual landslide points\nvar realLabeled = susceptibilityZones.sampleRegions({\n  collection: landslidePoints.map(function(f) {\n    return f.set('label', 1); // positive class\n  }),\n  scale: 1000\n});\n\n// Assign class = 0 to random control points (no landslide)\nvar randomLabeled = susceptibilityZones.sampleRegions({\n  collection: randomPoints.map(function(f) {\n    return f.set('label', 0); // Negative class\n  }),\n  scale: 1000\n});\n\n// Combine all labeled points into one dataset\nvar allPoints = realLabeled.merge(randomLabeled);\n\n// Generate binary predictions:\n// Predicted = 1 if Zone 3 (high-risk); else 0\nvar predictions = allPoints.map(function(f) {\n  var zone = ee.Number(f.get('zone'));\n  var predicted = zone.eq(3); // binary risk\n  return f.set('predicted', predicted);\n});\n\n// Compute confusion matrix: compares predicted vs actual labels\nvar confusion = predictions.errorMatrix('label', 'predicted');\n\n// Extract underlying array from the error matrix\n// [[TN, FP],\n//  [FN, TP]]\nvar cmArray = ee.Array(confusion.array());\n\n// Extract key elements from the matrix:\nvar TP = cmArray.get([1, 1]); // True Positive: correctly predicted landslide\nvar FP = cmArray.get([0, 1]); // False Positive: predicted landslide but no event\nvar FN = cmArray.get([1, 0]); // False Negative: missed actual landslide\n\n// Compute Precision (Producer's Accuracy):\n// Of all predicted landslides, what proportion were actual landslides?\nvar precision = TP.divide(TP.add(FP));\n\n// Compute Recall (Consumer's Accuracy):\n// Of all actual landslides, what proportion were correctly predicted?\nvar recall = TP.divide(TP.add(FN));\n\n// Compute F1 Score:\n// Harmonic mean of precision and recall, balances false positives and false negatives\n// .max(1) avoids division by zero\nvar f1 = precision.multiply(recall).multiply(2).divide(precision.add(recall).max(1));\n\n\n\n// Accuracy = 0.59 - The model correctly classified 59% of all validation points (landslide + non-landslide)\n// Precision = 0.61 - Of all 1km² areas predicted as high-risk, 61% had a landslide\n// Recall = 0.50 - The model correctly identified 50% of actual landslides (some were missed, falling in lower-risk zones)\n// F1 Score = 0.55 - A balanced metric combining precision and recall, reflecting moderate overall predictive performance\n\n\n// 5. Random Forest Classifier\n// Assign class = 1 for known landslide points\nvar landslideLabeled = landslidePoints.map(function(f) {\n  return f.set('class', 1); // positive class (landslide occurred)\n});\n\n// Generate random negative points with class = 0\nvar nonLandslidePoints = ee.FeatureCollection.randomPoints({\n  region: NepalBoundary.geometry(),\n  points: landslidePoints.size(), // match number as positive class for balanced training\n  seed: 42 // set seed for reproducibility\n}).map(function(f) {\n  return f.set('class', 0); // negative class (assumed no landslide)\n});\n\n// Merge positive and negative samples into one training dataset\nvar trainingPoints = landslideLabeled.merge(nonLandslidePoints);\n\n// Build feature stack of predictors\nvar featuresImage = ee.Image.cat([\n  normSlope, // slope\n  normalizedDrainageDensity, // drainage density\n  // normalizedDistance, // optional\n  normElev, // elevation\n  normClay, // clay content\n  reclassifiedLand, // reclassified MODIS land cover\n  normalizedSoilTexture // reclassified soil texture\n]).rename([\n  'slope', 'drainDen', 'elev', 'clay', 'landcover', 'soiltex'\n]);\n\n// Sample predictor values at point locations\nvar training = featuresImage.sampleRegions({\n  collection: trainingPoints,\n  properties: ['class'], // target label\n  scale: 1000 // match input raster resolution\n});\n\n// Train a Random Forest Classifier (50 trees)\nvar classifier = ee.Classifier.smileRandomForest(50).train({\n  features: training,\n  classProperty: 'class',\n  inputProperties: ['slope', 'drainDen', 'elev', 'clay', 'landcover', 'soiltex']\n});\n\n// Apply classifier to raster to generate prediction map\nvar classified = featuresImage.classify(classifier);\nMap.addLayer(classified, {min: 0, max: 1, palette: ['white', 'red']}, 'RF Landslide Prediction', false);\n// white = low probability, red = high probability of landslide occurrences\n\n// Create a probability classifier by setting output mode to PROBABILITY\nvar probabilityClassifier = classifier.setOutputMode('PROBABILITY');\n\n// Classify the image with the probability classifier\nvar probabilityMap = featuresImage.classify(probabilityClassifier);\n\n// Visualise the probability map (0 = low risk, 1 = high risk)\n// This is more interpretable than just a binary yes/no map\nMap.addLayer(probabilityMap, {\n  min: 0,\n  max: 1,\n  palette: ['white', 'yellow', 'orange', 'red']\n}, 'RF Landslide Probability', false);\n\n// Accuracy Assessment\n// Split training data into train/test sets using a random column\nvar withRandom = training.randomColumn('random'); // adds column 'random' between 0-1\nvar split = 0.7;\nvar trainSet = withRandom.filter(ee.Filter.lt('random', split)); // 70% for training\nvar valSet = withRandom.filter(ee.Filter.gte('random', split)); // 30% for validation\n\n// Re-train on 70% subset\nvar trainedRF = ee.Classifier.smileRandomForest(50).train({\n  features: trainSet,\n  classProperty: 'class',\n  inputProperties: ['slope', 'drainDen', 'elev', 'clay', 'landcover', 'soiltex']\n});\n\n// Filter out validation points with any null feature\n// Classify the validation set using the re-trained RF model\nvar validated = valSet.filter(ee.Filter.notNull(['slope', 'drainDen', 'elev', 'clay', 'landcover', 'soiltex']))\n                      .classify(trainedRF);\n                      \n// generate a confusion matrix by comparing true labels with predicted ones\nvar rfMatrix = validated.errorMatrix('class', 'classification');\n// Extract array from RF confusion matrix\nvar rfArray = ee.Array(rfMatrix.array());\n\n// Extract individual values for binary classification\nvar TP_rf = rfArray.get([1, 1]); // True Positive\nvar FP_rf = rfArray.get([0, 1]); // False Positive\nvar FN_rf = rfArray.get([1, 0]); // False Negative\n\n// Precision = TP / (TP + FP)\n// Of all areas predicted as high-risk, how many actually had landslides?\nvar precision_rf = TP_rf.divide(TP_rf.add(FP_rf));\n// Recall = TP / (TP + FN)\n// Of all actual landslides, how many were correctly predicted as high-risk?\nvar recall_rf = TP_rf.divide(TP_rf.add(FN_rf));\n// F1 Score = 2 * (precision * recall) / (precision + recall)\n// Balances the model’s ability to avoid false positives and false negatives\nvar f1_rf = precision_rf.multiply(recall_rf).multiply(2).divide(precision_rf.add(recall_rf).max(1));\n\n// Variable Importance in RF Classifier (how useful each variable was in tree splits)\nvar importance = ee.Dictionary(classifier.explain().get('importance'));\n\n// Convert variable importance dictionary to a FeatureCollection\nvar keys = importance.keys();\n\nvar importanceFeatures = keys.map(function(key) {\n  key = ee.String(key);\n  return ee.Feature(null, {\n    variable: key,\n    importance: ee.Number(importance.get(key))\n  });\n});\n\n// Sort features by descending importance\nvar importanceFC = ee.FeatureCollection(importanceFeatures)\n  .sort('importance', false);\n\n// Build a horizontal bar chart usig the FeatureCollection\nvar chart = ui.Chart.feature.byFeature(importanceFC, 'variable', 'importance')\n  .setChartType('BarChart')\n  .setOptions({\n    title: 'Random Forest Variable Importance',\n    hAxis: {title: 'Importance'}, // x-axis: importance score\n    vAxis: {title: 'Variable'}, // y-axis: variable names\n    legend: 'none',\n    fontSize: 12\n  });\n\n// ---------------------------------------------------------------------------------------------------\n\n// Model Verification for Populated Areas Only (on damaged landslide points in populated areas) - Added 29/04/2025\n\n// 1. Filter damaged landslides to only populated areas\nvar populatedMask = popBinary.selfMask();  // mask for populated pixels\nvar landslideInPopulated = landslidePoints.filterBounds(populatedMask.geometry());\n\nvar filteredPoints = popBinary.reduceRegions({\n  collection: landslideInPopulated,\n  reducer: ee.Reducer.first(),\n  scale: 1000\n}).filter(ee.Filter.eq('first', 1));  // keep only if pixel itself is populated\n\nMap.addLayer(filteredPoints, {color: 'purple', pointSize: 5}, 'Landslides Points (Populated Areas)');\n\n// 2. Sample susceptibility risk at those landslide points (in populated areas)\nvar populatedValidationSamples = normalizedPopulatedRisk.sampleRegions({\n  collection: filteredPoints,\n  scale: 1000,\n  geometries: true\n}).filter(ee.Filter.neq('risk', null));\n\nvar populatedPointRiskCounts = populatedValidationSamples.reduceColumns({\n  selectors: ['risk'],\n  reducer: ee.Reducer.histogram()\n});\n\n// 3. Compute population-specific percentiles\nvar populatedRiskPercentiles = normalizedPopulatedRisk.reduceRegion({\n  reducer: ee.Reducer.percentile([33, 66]),\n  geometry: NepalBoundary.geometry(),\n  scale: 1000,\n  bestEffort: true\n});\nvar p33Pop = ee.Number(populatedRiskPercentiles.get('risk_p33'));\nvar p66Pop = ee.Number(populatedRiskPercentiles.get('risk_p66'));\n\n\n// 4. Reclassify risk map into zones (1 = low, 2 = medium, 3 = high)\nvar populatedSusceptibilityZones = normalizedPopulatedRisk\n  .where(normalizedPopulatedRisk.lt(p33Pop), 1)\n  .where(normalizedPopulatedRisk.gte(p33Pop).and(normalizedPopulatedRisk.lt(p66Pop)), 2)\n  .where(normalizedPopulatedRisk.gte(p66Pop), 3)\n  .rename('zone')\n  .updateMask(normalizedPopulatedRisk.mask())\n  .clip(NepalBoundary);\n\nMap.addLayer(populatedSusceptibilityZones, {min: 1, max: 3, palette: ['green', 'yellow', 'red']}, 'Populated Susceptibility Zones', true);\n\n// 5. Sample zones at landslide points\nvar populatedValidationZones = populatedSusceptibilityZones.sampleRegions({\n  collection: filteredPoints,\n  scale: 1000,\n  geometries: true\n}).filter(ee.Filter.neq('zone', null));\n\nvar populatedPointCounts = populatedValidationZones.reduceColumns({\n  selectors: ['zone'],\n  reducer: ee.Reducer.frequencyHistogram()\n});\n\n// 6. Generate random points in Nepal and retain only those in populated pixels\nvar randomPopulatedPoints = ee.FeatureCollection.randomPoints({\n  region: NepalBoundary.geometry(),\n  points: filteredPoints.size(),\n  seed: 42,\n  maxError: 1\n}).filterBounds(popBinary.geometry());\n\nvar randomPopulatedSamples = populatedSusceptibilityZones.sampleRegions({\n  collection: randomPopulatedPoints,\n  scale: 1000,\n  geometries: true\n}).filter(ee.Filter.neq('zone', null));\n\n// 7. Frequency histogram comparison\nvar realPopulatedCounts = populatedValidationZones.reduceColumns({\n  selectors: ['zone'],\n  reducer: ee.Reducer.frequencyHistogram()\n});\nvar randomPopulatedCounts = randomPopulatedSamples.reduceColumns({\n  selectors: ['zone'],\n  reducer: ee.Reducer.frequencyHistogram()\n});\n\n// 8. Chi-Square test: real vs random distribution\nvar realPopDict = ee.Dictionary(realPopulatedCounts.get('histogram'));\nvar randomPopDict = ee.Dictionary(randomPopulatedCounts.get('histogram'));\nvar keysPop = realPopDict.keys();\n\nvar chiSquarePop = keysPop.map(function(key) {\n  key = ee.String(key);\n  var observed = ee.Number(realPopDict.get(key));\n  var expected = ee.Number(randomPopDict.get(key)).max(1);\n  return observed.subtract(expected).pow(2).divide(expected);\n});\nvar chiSquareTotalPop = chiSquarePop.reduce(ee.Reducer.sum());\n\n// Binary Evaluation for Populated Areas Only\n\nvar populatedRealLabeled = populatedSusceptibilityZones.sampleRegions({\n  collection: filteredPoints.map(function(f) {\n    return f.set('label', 1);\n  }),\n  scale: 1000,\n  geometries: true\n}).filter(ee.Filter.neq('zone', null));\n\nvar populatedRandomLabeled = populatedSusceptibilityZones.sampleRegions({\n  collection: randomPopulatedPoints.map(function(f) {\n    return f.set('label', 0);\n  }),\n  scale: 1000,\n  geometries: true\n}).filter(ee.Filter.neq('zone', null));\n\nvar populatedAllPoints = populatedRealLabeled.merge(populatedRandomLabeled);\n\nvar populatedPredictions = populatedAllPoints.map(function(f) {\n  var zone = ee.Number(f.get('zone'));\n  var predicted = zone.eq(3);  // high risk\n  return f.set('predicted', predicted);\n});\n\nvar populatedConfusion = populatedPredictions.errorMatrix('label', 'predicted');\n\nvar cmArrayPop = ee.Array(populatedConfusion.array());\nvar TP_pop = cmArrayPop.get([1, 1]);\nvar FP_pop = cmArrayPop.get([0, 1]);\nvar FN_pop = cmArrayPop.get([1, 0]);\n\nvar precisionPop = TP_pop.divide(TP_pop.add(FP_pop));\nvar recallPop = TP_pop.divide(TP_pop.add(FN_pop));\nvar f1Pop = precisionPop.multiply(recallPop).multiply(2).divide(precisionPop.add(recallPop).max(1));\n\n// Model Comparison Summary:\n// - Chi-square value more than tripled (804.6 → 2562.9), indicating a much stronger statistical association between predicted high-risk zones and actual landslides occurring in populated areas.\n// - Precision significantly improved (0.600 → 0.729), meaning the model's high-risk predictions are more trustworthy where people live.\n// - Recall declined (0.504 → 0.480), indicating that a portion of actual landslides in populated areas still fall outside the predicted high-risk zone.\n// - F1 Score increased (0.548 → 0.579), reflecting better overall balance between precision and recall.\n// - Accuracy slightly declined (0.581 → 0.537), which is expected as unpopulated, low-risk areas (which are easy to classify) are excluded.\n// Limiting evaluation to populated areas enhances the model’s relevance and precision for human-impact zones\n// even if some events remain harder to capture. This prioritises decision-making where exposure is highest.\n\n// Random Forest Classifier (Populated Areas Only)\n\n// 1. Label landslide points\nvar populatedLandslidePoints = filteredPoints.map(function(f) {\n  return f.set('class', 1);\n});\n\n// 2. Compute oversample factor based on area\nvar nepalArea = NepalBoundary.geometry().area().divide(1e6);  // in km²\nvar populatedPixelCount = popBinary.selfMask().reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: NepalBoundary.geometry(),\n  scale: 1000,\n  maxPixels: 1e13\n}).get('popBinary');\nvar populatedArea = ee.Number(populatedPixelCount);  // each pixel ~ 1 km²\nvar oversampleFactor = nepalArea.divide(populatedArea).ceil();\n\n// 3. Generate oversampled random points and mask to populated areas\nvar oversampledPoints = ee.FeatureCollection.randomPoints({\n  region: NepalBoundary.geometry(),\n  points: populatedLandslidePoints.size().multiply(oversampleFactor).int(),\n  seed: 42,\n  maxError: 1\n});\n\nvar randomPopulatedPoints = popBinary.reduceRegions({\n  collection: oversampledPoints,\n  reducer: ee.Reducer.first(),\n  scale: 1000\n}).filter(ee.Filter.eq('first', 1))\n  .limit(populatedLandslidePoints.size())\n  .map(function(f) {\n    return f.set('class', 0);\n  });\n\n// 4. Visual confirmation\nMap.addLayer(populatedLandslidePoints, {color: 'red'}, 'Populated Landslide Points');\nMap.addLayer(randomPopulatedPoints, {color: 'blue'}, 'Random Points in Populated Areas', false);\n\n// 5. Training data and feature stack\nvar trainingPointsPopulated = populatedLandslidePoints.merge(randomPopulatedPoints);\n\nvar featuresImagePopulated = ee.Image.cat([\n  normSlope,\n  normalizedDrainageDensity,\n  normElev,\n  normClay,\n  reclassifiedLand,\n  normalizedSoilTexture\n]).rename([\n  'slope', 'drainDen', 'elev', 'clay', 'landcover', 'soiltex'\n]).updateMask(popBinary);\n\nvar trainingPopulated = featuresImagePopulated.sampleRegions({\n  collection: trainingPointsPopulated,\n  properties: ['class'],\n  scale: 1000,\n  geometries: true\n});\n\n// 6. Train/test split\nvar withRandom = trainingPopulated.randomColumn('random');\nvar trainSetPop = withRandom.filter(ee.Filter.lt('random', 0.7));\nvar valSetPop = withRandom.filter(ee.Filter.gte('random', 0.7));\n\n// 7. Train Random Forest model\nvar rfClassifierPop = ee.Classifier.smileRandomForest(50).train({\n  features: trainSetPop,\n  classProperty: 'class',\n  inputProperties: ['slope', 'drainDen', 'elev', 'clay', 'landcover', 'soiltex']\n});\n\n// 8. Evaluate on validation set\nvar validatedPop = valSetPop\n  .filter(ee.Filter.notNull(['slope', 'drainDen', 'elev', 'clay', 'landcover', 'soiltex']))\n  .classify(rfClassifierPop);\n\nvar rfMatrixPop = validatedPop.errorMatrix('class', 'classification');\nvar rfArrayPop = ee.Array(rfMatrixPop.array());\n\nvar TP_rfPop = rfArrayPop.get([1, 1]);\nvar FP_rfPop = rfArrayPop.get([0, 1]);\nvar FN_rfPop = rfArrayPop.get([1, 0]);\n\nvar precision_rfPop = TP_rfPop.divide(TP_rfPop.add(FP_rfPop));\nvar recall_rfPop = TP_rfPop.divide(TP_rfPop.add(FN_rfPop));\nvar f1_rfPop = precision_rfPop.multiply(recall_rfPop).multiply(2).divide(precision_rfPop.add(recall_rfPop).max(1));\n\n// 9. Apply classifier to image\nvar classifiedPopulated = featuresImagePopulated.classify(rfClassifierPop);\nMap.addLayer(classifiedPopulated, {min: 0, max: 1, palette: ['white', 'red']}, 'RF Prediction (Populated Areas)');\n\n// 10. Generate probability surface\nvar probabilityClassifierPop = rfClassifierPop.setOutputMode('PROBABILITY');\nvar probabilityMapPop = featuresImagePopulated.classify(probabilityClassifierPop);\nMap.addLayer(probabilityMapPop, {\n  min: 0,\n  max: 1,\n  palette: ['white', 'yellow', 'orange', 'red']\n}, 'RF Probability (Populated Areas)');\n\n// 11. Variable importance\nvar importancePop = ee.Dictionary(rfClassifierPop.explain().get('importance'));\n\nvar importanceFeaturesPop = importancePop.keys().map(function(key) {\n  key = ee.String(key);\n  return ee.Feature(null, {\n    variable: key,\n    importance: ee.Number(importancePop.get(key))\n  });\n});\n\nvar importanceFC = ee.FeatureCollection(importanceFeaturesPop).sort('importance', false);\nvar chartPop = ui.Chart.feature.byFeature(importanceFC, 'variable', 'importance')\n  .setChartType('BarChart')\n  .setOptions({\n    title: 'Variable Importance (Populated Areas)',\n    hAxis: {title: 'Importance'},\n    vAxis: {title: 'Variable'},\n    legend: 'none',\n    fontSize: 12\n  });"
  },
  {
    "objectID": "index.html#interface",
    "href": "index.html#interface",
    "title": "Nepal Landslides Risk Application",
    "section": "Interface",
    "text": "Interface\nThe website provides an intuitive, multilingual interface combining expertise and data into robust models, offering comparative visualizations and statistical summaries. The platform prioritizes clarity through map-based dashboards and toggleable language options to enhance accessibility and local impact.\nAs discussed in detail in Section ‘How It Works, 2. Interactive Visualization and Analysis’.\n\n\n\n\n\nDistrict Level View\n\n\n\n\n\n\n\n\nMethodology View"
  },
  {
    "objectID": "index.html#the-application",
    "href": "index.html#the-application",
    "title": "Nepal Landslides Risk Application",
    "section": "The Application",
    "text": "The Application\n\nDemo\nThe application runs on Google Earth Engine, allowing users to visualise landslide risk across Nepal dynamically. Users can zoom in to specific districts, interpret risk zones, and overlay incident data to validate model predictions. The tool is built for scalability and may be adapted for other regions facing similar geological threats.\n\n\n\n\n\n\nKey Features Beyond the Demo\nWhile our demo showcases the app’s core functionality, several critical features work behind the scenes to enhance its performance and reliability. In this section, we highlight key capabilities implemented to optimize efficiency and ensure a seamless user experience.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Language Support\nDesigned with the end user in mind, the app supports Nepal’s primary languages—Nepali (~44%), Maithili (~12%), and Bhojpuri (~6%)—ensuring accessibility for diverse users, including those with low English fluency.\n\n\n\n\n\n\n// Language selection dropdown\nvar languageSelect = ui.Select({\nitems: [\n{label: 'English', value: 'English'},\n{label: 'Nepali', value: 'Nepali'},\n{label: 'Maithili', value: 'Maithili'},\n{label: 'Bhojpuri', value: 'Bhojpuri'}\n],\nonChange: function(value) {\nresetToNationalView();\nupdateLanguage(value);\n}\n});\n/ Translation dictionary (Sample)\nvar LANGUAGE_TRANSLATIONS = {\n'Nepali': { 'Click on the map or select from list:': 'नक्सामा क्लिक गर्नुहोस् वा सूचीबाट चयन गर्नुहोस्:' },\n'Maithili': { 'Click on the map or select from list:': 'नक्शा पर क्लिक करू या सूची सँ चुनू' },\n'Bhojpuri': { 'Click on the map or select from list:': 'नक्शा प क्लिक करीं या सूची से चुनीं' }\n}; \n\n\n\nEfficient Caching\nTo avoid redundant computations, the app Leverages local memory for server-side calculations, ensuring responsiveness even with large datasets.\n\n\n\n\n\n\n// Cache national statistics at startup\nee.Dictionary({\nincidentCount: districtFactors.aggregate_sum('incidentCount'),\ndeaths: districtFactors.aggregate_sum('deaths'),\ninjuries: districtFactors.aggregate_sum('injuries'),\ninfrastructureDestroyed: districtFactors.aggregate_sum('infraDestroyed')\n}).evaluate(\nfunction(values) {\n// Cached values used in national overview for quick loading\nnationalStats.incidentCount = Number(values.incidentCount) || 0; // Total incidents\nnationalStats.deaths = Number(values.deaths) || 0; // Total deaths\nnationalStats.injuries = Number(values.injuries) || 0; // Total injuries\nnationalStats.infrastructureDestroyed = Number(values.infrastructureDestroyed) || 0; // Total infrastructure damage\ninitializeAppAfterCaching();\n}\n);\n\n\n\nRobust Error Handling\nThe app handles clicks outside study areas and data issues with clear messages to keep users informed and engaged, preventing crashes and enhancing reliability.\n\n\n\n\n\n\n// Handle clicks outside Nepal\nNepalBoundary.geometry().contains(point).evaluate(function(isInside) {\nif (!isInside) {\nStatisticsPanel.clear();\nvar widgets = controlPanel.widgets();\nif (widgets.indexOf(districtInstructionLabel) !== -1) {\nwidgets.remove(districtInstructionLabel);\n}\nif (widgets.indexOf(districtSelectPanel) !== -1) {\nwidgets.remove(districtSelectPanel);\n}\nStatisticsPanel.add(ui.Label(translate('Click outside Nepal boundary. Please select a point within Nepal.'), STYLES.SUBTITLE));\nStatisticsPanel.add(ui.Button({\nlabel: translate('Reset to National Overview'),\nonClick: function() {\nresetToNationalView();\n},\nstyle: { margin: '10px auto', stretch: 'horizontal', textAlign: 'center' }\n}));\n} });\n\n\n\nOptimal Rendering\nThe app enhances speed and user experience by clipping and rendering high-resolution raster data for the selected district only, rather than the entire country.\n\n\n\n\n\n\n// Highlight and clip layers for the selected district\nfunction highlightDistrict(district) {\n// Simplify geometry to reduce computation\nvar districtGeometry = district.geometry().simplify({maxError: 100});\n// Clip layers to district for efficient rendering\nvar clippedLandslide = landslidePoints.filterBounds(districtGeometry);\nvar clippedSus = LandslideSusceptibility.clip(districtGeometry);\n\nclippedLandslideFeatures = null;\nclippedLandslide.evaluate(function(features) {\nclippedLandslideFeatures = features.features || [];\n});\n\nclearMapLayersExceptHighlight();\n// Add clipped layers to map\nvar susceptibilityLayerclip = ui.Map.Layer(clippedSus, {min: 0, max: 1, palette: [STYLES.COLORS.LOW_RISK, STYLES.COLORS.MEDIUM_RISK, STYLES.COLORS.HIGH_RISK]});\nMap.layers().add(susceptibilityLayerclip);\n}\n\n\n\nCustom Basemaps and Streamlined Controls\nThe app enhances the user experience by customizing the basemap to focus on essential geographic data, modifying road displays (e.g., highways, arterial, and local roads) to reduce visual clutter and removing unneeded base elements like saturation. Additionally, main map controls (e.g., zoom, map type, scale, fullscreen) are hidden to provide a distraction-free interface.\n\n\n\n\n\n\n// Customize basemap to reduce visual clutter\nvar roadNetwork = [\n{stylers: [{saturation: -100}]}, // Desaturate basemap for focus on data layers\n{featureType: 'road.highway', elementType: 'geometry.fill', stylers: [{color: '#000055'}, {weight: 0.1}]}, // Simplify highway display\n{featureType: 'road.highway', elementType: 'geometry.stroke', stylers: [{color: '#000000'}, {weight: 0.1}]},\n{featureType: 'road.arterial', elementType: 'geometry', stylers: [{color: '#FF0000'}, {weight: 0.1}]}, // Simplify arterial roads\n{featureType: 'road.local', elementType: 'geometry', stylers: [{color: '#00FF55'}, {weight: 0.1}]} // Simplify local roads\n];\nMap.setOptions('roadNetwork', { roadNetwork: roadNetwork });\n\n// Hide main map controls for a distraction-free interface\nMap.setControlVisibility({\nzoomControl: false, // Disable zoom control\nmapTypeControl: false, // Disable map type selector\nscaleControl: false, // Disable scale control\nfullscreenControl: false // Disable fullscreen control\n});\nMap.drawingTools().setShown(false); // Hide drawing tools\n\n\n\n\nSummary Popups\nClicking a landslide point in district view displays a popup with aggregated details, aiding understanding of past events at specific locations.\n\n\n\n\n\n\n// Detect landslide points within buffer and aggregate data\nvar clickRadius = 0.0027; // ~300 meters\nvar matchingFeatures = [];\nclippedLandslideFeatures.forEach(function(feature) {\nvar featureCoords = feature.geometry.coordinates;\nvar distance = Math.sqrt(\nMath.pow(clickLon - featureLon, 2) + Math.pow(clickLat - featureLat, 2)\n);\nif (distance &lt; clickRadius) {\nmatchingFeatures.push(feature);\n}\n});\n\nif (matchingFeatures.length &gt; 0) {\nvar totalDeaths = 0, totalInjuries = 0, incidentCount = matchingFeatures.length;\nmatchingFeatures.forEach(function(feature) {\nvar props = feature.properties || {};\ntotalDeaths += props.peopleDeathCount || 0;\ntotalInjuries += props.peopleInjuredCount || 0;\n});\nshowLandslidePopup({ incidentCount: incidentCount, totalDeaths: totalDeaths, totalInjuries: totalInjuries });\n}\n\n// Display aggregated data in popup\nfunction showLandslidePopup(aggregatedData, coords) {\nvar titleTemplate = translate('Landslide Events ({0} Incidents)');\nvar title = titleTemplate.replace('{0}', aggregatedData.incidentCount);\ncontentPanel.add(ui.Label(title, { fontSize: '16px', fontWeight: 'bold' }));\ncontentPanel.add(createRow('Deaths:', aggregatedData.totalDeaths));\ncontentPanel.add(createRow('Injuries:', aggregatedData.totalInjuries));\n}\n\n\n\nInteractive Analysis\nUsers can interact with districts by clicking on the map, selecting from a dropdown, or using connected charts, enabling seamless data exploration.\n\n\n\n\n\n\n// Chart interaction in national overview\ntopDistrictsChart.onClick(function(district) {\nif (district) {\nvar originalDistrictName = district.toUpperCase();\nselectDistrict(originalDistrictName);\n}\n});\n\n// Alternative: District dropdown selection\ndistrictSelect = ui.Select({\nitems: districtSelectItems,\nplaceholder: translate('Select District:'),\nonChange: function(value) {\nif (value === 'National Overview') {\nresetToNationalView();\n} else {\nselectDistrict(value);\n}\n}\n});\n\n// Alternative: Map click selection\nMap.onClick(function(coords) {\nvar point = ee.Geometry.Point(coords.lon, coords.lat);\nvar clickedDistrict = districts.filterBounds(point).first();\nif (clickedDistrict) {\nclickedDistrict.get('DISTRICT').evaluate(function(districtName) {\ndistrictName = districtName.trim().toUpperCase();\nselectDistrict(districtName);\n});\n} });\n\n\n\n\n\nPrevious\n\n\n\nNext"
  },
  {
    "objectID": "index.html#how-it-works",
    "href": "index.html#how-it-works",
    "title": "Nepal Landslides Risk Application",
    "section": "How it Works",
    "text": "How it Works\n\n1. Data Preparation and Precomputations\nTo handle complex computations, separate scripts were developed for the two methodologies explained above. Results are imported into the app as precomputed layers.\n\nImported Precomputed Layers:\n\nEnvironmental Landslide Susceptibility: Maps susceptibility using environmental variables.\nNormalized Landslide Susceptibility: Adjusted for population exposure.\nRandom Forest Landslide Probability: ML-based risk focused on populated areas.\n\n\n\nDistrict-Level Aggregation:\nA dedicated script was built to aggregate historical reported landslide data with impact metrics (e.g., incidents, deaths, injuries, destroyed infrastructure), alongside population and susceptibility metrics per district. It also includes engineered metrics such as incidents per km² and nearest districts (determined via spatial proximity of centroids) for comparative analysis.\n  // Compute distances to other districts and find two nearest neighbors using precomputed centroids\n  var districtsWithDistance = districtFactors.map(function(otherDistrict) {\n    var otherName = otherDistrict.get('DISTRICT');\n    var lon2 = ee.Number(otherDistrict.get('centroid_lon'));\n    var lat2 = ee.Number(otherDistrict.get('centroid_lat'));\n\n    // Approximate distance using Euclidean distance in lat/lon \n    var distance = lon1.subtract(lon2).pow(2).add(lat1.subtract(lat2).pow(2)).sqrt();\n    return otherDistrict.set('distance', distance);\n  });\n\n  var nearbyDistricts = districtsWithDistance\n    .filter(ee.Filter.neq('DISTRICT', districtName))\n    .sort('distance')\n    .limit(2);\n\n  // Extract the two nearest neighbors\n  var neighbor1 = nearbyDistricts.first();\n  var neighbor2 = nearbyDistricts.toList(2).get(1);\n\n\n\n2. Interactive Visualization and Analysis\n\nNational Overview\nAt startup, national-level statistics are displayed, highlighting aggregated data. A pie chart shows population risk distribution across susceptibility zones (Low, Medium, High), and a bar chart lists the top 5 districts by impact metric, allowing users to switch metrics and zoom into a district by clicking a bar.\n\n\nSusceptibility Zones\nSusceptibility zones are defined as Low (&lt;0.3), Medium (0.3–0.5), and High (&gt;0.5), identified by analyzing the distribution of susceptibility values from the built Susceptibility model.\n\n// Generate a histogram of susceptibility values\nvar histogram = LandslideSusceptibility.reduceRegion({\n  reducer: ee.Reducer.histogram({min: 0, max: 1, steps: 100}),\n  geometry: NepalBoundary.geometry(),\n  scale: 1000, // Match the scale used in the app for consistency\n  maxPixels: 1e10,\n  bestEffort: true\n});\n\n// Extract the histogram data\nvar histogramDict = ee.Dictionary(histogram.get('susceptibility'));\nvar values = ee.List(histogramDict.get('histogram')); // Frequency counts\nvar bucketEdges = ee.List(histogramDict.get('bucketMeans')); // Bin centers\n\n\n// Define susceptibility zones based on histogram-derived thresholds\nvar thresholdZones = LandslideSusceptibility\n  .where(LandslideSusceptibility.lt(0.3), 1) // Low Risk: &lt; 0.3\n  .where(LandslideSusceptibility.gte(0.3).and(LandslideSusceptibility.select('slope').lte(0.5)), 2) // Medium Risk: 0.3–0.5\n  .where(LandslideSusceptibility.gt(0.5), 3) // High Risk: &gt; 0.5\n  .rename('zone')\n  .updateMask(LandslideSusceptibility.mask())\n  .clip(NepalBoundary);\n\n\nDistrict-Level Analysis\nThis view provides detailed district analysis, showing key metrics like average susceptibility, population, and incidents per km², alongside comparisons with national averages and the two nearest districts. Historical landslides are visualized on the map, with popups displaying aggregated details within a 300m radius of a clicked point.\n\n\nMethodology Overview\nThis view allows users to compare the two methodologies (Environmental Susceptibility and RF) using a split panel. It displays the RF probability map and normalized risk map for populated areas, with a draggable divider for spatial comparison, focusing on population exposure.\n// Initialize split maps with layers\nvar leftMap = ui.Map();\nvar rightMap = ui.Map();\n\n// Add risk layers to split maps\nvar leftProbabilityLayer = ui.Map.Layer(probabilityLayer.getEeObject(), probabilityLayer.getVisParams(), translate('RF Landslide Probability (Populated Areas)'));\nleftMap.layers().add(leftProbabilityLayer);\nvar rightNormalizedRiskLayer = ui.Map.Layer(normalizedRiskLayer.getEeObject(), normalizedRiskLayer.getVisParams(), translate('Landslide Susceptibility (Populated Areas)'));\nrightMap.layers().add(rightNormalizedRiskLayer);\n\n// Link maps for synchronized zooming and panning\nvar mapLinker = ui.Map.Linker([leftMap, rightMap]);\n\n// Set up the split panel for comparison\nvar splitPanel = ui.SplitPanel({\n firstPanel: leftMap,\n secondPanel: rightMap,\n orientation: 'horizontal',\n wipe: true\n});\n\n\n\n3. Insights and Objectives\nThe app provides actionable insights for disaster preparedness in Nepal by identifying high-risk districts and population exposure. It supports:\n\nRisk Prioritization: Highlights districts with high susceptibility or incident rates (e.g., top 5 districts by deaths) to guide resource allocation.\nLocalized Insights: Enables district-level analysis for targeted planning and response, benefiting community preparedness efforts.\nAcademic Research: Unlike traditional landslide models in academic papers, this app provides an interactive platform for exploring risk dynamics through a comparison of Environmental Susceptibility and Random Forest methodologies, leveraging incidents data. By masking to populated areas, it focuses on human exposure, as official records often under-report incidents in uninhabited regions, ensuring more accurate risk assessment and model comparison for populated zones.\n\n\n\n4. Limitations and Potential Expansion\n\nStatic Models: Current models are precomputed, limiting adaptability to changing environmental conditions. Adding dynamic updates with live data (e.g., rainfall, land use changes) could improve accuracy and responsiveness.\nGranularity of Analysis: The app focuses on district-level analysis, but more granular insights (e.g., at the village or ward level) could improve localized planning.\nReport Export: The app lacks a feature to export analysis reports, which would be valuable for stakeholders to document and share findings."
  }
]